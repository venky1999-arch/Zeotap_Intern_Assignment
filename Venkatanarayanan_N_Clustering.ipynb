{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35b9ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.decomposition import PCA\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb72b5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis complete! Check the following files:\n",
      "1. plots/ directory for visualizations\n",
      "2. cluster_analysis_report.txt for detailed analysis\n",
      "3. customer_segments.csv for cluster assignments\n"
     ]
    }
   ],
   "source": [
    "def davies_bouldin_index(X, labels):\n",
    "    \"\"\"\n",
    "    Calculate Davies-Bouldin Index\n",
    "    Lower values indicate better clustering\n",
    "    \"\"\"\n",
    "    n_clusters = len(np.unique(labels))\n",
    "    cluster_centers = np.array([X[labels == i].mean(axis=0) for i in range(n_clusters)])\n",
    "    \n",
    "    # Calculate cluster dispersions (average distance to center)\n",
    "    dispersions = np.zeros(n_clusters)\n",
    "    for i in range(n_clusters):\n",
    "        if sum(labels == i) > 0:\n",
    "            dispersions[i] = np.mean(cdist(X[labels == i], [cluster_centers[i]]))\n",
    "    \n",
    "    # Calculate Davies-Bouldin Index\n",
    "    db_index = 0\n",
    "    for i in range(n_clusters):\n",
    "        if dispersions[i] == 0:\n",
    "            continue\n",
    "        max_ratio = 0\n",
    "        for j in range(n_clusters):\n",
    "            if i != j and dispersions[j] > 0:\n",
    "                ratio = (dispersions[i] + dispersions[j]) / \\\n",
    "                        max(1e-5, np.linalg.norm(cluster_centers[i] - cluster_centers[j]))\n",
    "                max_ratio = max(max_ratio, ratio)\n",
    "        db_index += max_ratio\n",
    "    \n",
    "    return db_index / n_clusters\n",
    "\n",
    "def prepare_features(customers_df, transactions_df):\n",
    "    \"\"\"\n",
    "    Prepare features for clustering by combining customer profiles and transaction data\n",
    "    \"\"\"\n",
    "    # First, let's identify numerical and categorical columns in customers_df\n",
    "    # We'll drop CustomerID as it's not a feature for clustering\n",
    "    customers_features = customers_df.drop('CustomerID', axis=1)\n",
    "    \n",
    "    # Handle categorical columns in customers_df\n",
    "    categorical_columns = customers_features.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    # Create dummy variables for categorical columns\n",
    "    for col in categorical_columns:\n",
    "        # For columns with many unique values, we might want to handle them differently\n",
    "        if customers_features[col].nunique() < 10:  # For columns with few unique values\n",
    "            customers_features = pd.get_dummies(customers_features, columns=[col], prefix=col)\n",
    "        else:  # For columns with many unique values, we might want to drop them\n",
    "            customers_features = customers_features.drop(col, axis=1)\n",
    "    \n",
    "    # Aggregate transaction data\n",
    "    transaction_features = transactions_df.groupby('CustomerID').agg({\n",
    "        'TransactionID': 'count',\n",
    "        'TotalValue': ['sum', 'mean', 'std']\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Flatten column names\n",
    "    transaction_features.columns = ['CustomerID', 'TransactionCount', \n",
    "                                  'TotalSpend', 'AverageSpend', 'SpendStd']\n",
    "    \n",
    "    # Merge customer features with transaction features\n",
    "    features_df = customers_df[['CustomerID']].merge(\n",
    "        transaction_features, \n",
    "        on='CustomerID', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Add the processed customer features\n",
    "    for col in customers_features.columns:\n",
    "        features_df[col] = customers_features[col]\n",
    "    \n",
    "    # Fill missing values\n",
    "    features_df = features_df.fillna(0)\n",
    "    \n",
    "    # Store feature names for later use\n",
    "    feature_names = features_df.columns[1:].tolist()  # Exclude CustomerID\n",
    "    \n",
    "    return features_df, feature_names\n",
    "\n",
    "def find_optimal_clusters(X, max_clusters=10):\n",
    "    \"\"\"\n",
    "    Find optimal number of clusters using elbow method and DB Index\n",
    "    \"\"\"\n",
    "    db_scores = []\n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    ch_scores = []\n",
    "    \n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        db_score = davies_bouldin_index(X, labels)\n",
    "        inertia = kmeans.inertia_\n",
    "        sil_score = silhouette_score(X, labels)\n",
    "        ch_score = calinski_harabasz_score(X, labels)\n",
    "        \n",
    "        db_scores.append(db_score)\n",
    "        inertias.append(inertia)\n",
    "        silhouette_scores.append(sil_score)\n",
    "        ch_scores.append(ch_score)\n",
    "    \n",
    "    return range(2, max_clusters + 1), db_scores, inertias, silhouette_scores, ch_scores\n",
    "\n",
    "def plot_clustering_metrics(k_range, db_scores, inertias, silhouette_scores, ch_scores):\n",
    "    \"\"\"\n",
    "    Plot clustering metrics for different numbers of clusters\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # DB Index\n",
    "    axes[0, 0].plot(k_range, db_scores, 'bo-')\n",
    "    axes[0, 0].set_xlabel('Number of Clusters')\n",
    "    axes[0, 0].set_ylabel('Davies-Bouldin Index')\n",
    "    axes[0, 0].set_title('Davies-Bouldin Index vs Number of Clusters')\n",
    "    \n",
    "    # Elbow plot\n",
    "    axes[0, 1].plot(k_range, inertias, 'ro-')\n",
    "    axes[0, 1].set_xlabel('Number of Clusters')\n",
    "    axes[0, 1].set_ylabel('Inertia')\n",
    "    axes[0, 1].set_title('Elbow Plot')\n",
    "    \n",
    "    # Silhouette score\n",
    "    axes[1, 0].plot(k_range, silhouette_scores, 'go-')\n",
    "    axes[1, 0].set_xlabel('Number of Clusters')\n",
    "    axes[1, 0].set_ylabel('Silhouette Score')\n",
    "    axes[1, 0].set_title('Silhouette Score vs Number of Clusters')\n",
    "    \n",
    "    # Calinski-Harabasz score\n",
    "    axes[1, 1].plot(k_range, ch_scores, 'mo-')\n",
    "    axes[1, 1].set_xlabel('Number of Clusters')\n",
    "    axes[1, 1].set_ylabel('Calinski-Harabasz Score')\n",
    "    axes[1, 1].set_title('Calinski-Harabasz Score vs Number of Clusters')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('clustering_metrics.png')\n",
    "    plt.close()\n",
    "\n",
    "def visualize_clusters(X, labels, feature_names, features_df):\n",
    "    \"\"\"\n",
    "    Create enhanced visualizations of the clusters\n",
    "    \"\"\"\n",
    "    # Create directory for plots if it doesn't exist\n",
    "    import os\n",
    "    if not os.path.exists('plots'):\n",
    "        os.makedirs('plots')\n",
    "    \n",
    "    # Add cluster labels to features_df\n",
    "    features_df = features_df.copy()  # Create a copy to avoid modifying the original\n",
    "    features_df['Cluster'] = labels\n",
    "    \n",
    "    # 1. PCA Visualization with improved styling\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', alpha=0.6)\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.title('Customer Segments Visualization (PCA)', pad=20)\n",
    "    \n",
    "    # Add cluster centers\n",
    "    cluster_centers_pca = pca.transform(\n",
    "        np.array([X[labels == i].mean(axis=0) for i in range(len(set(labels)))])\n",
    "    )\n",
    "    plt.scatter(cluster_centers_pca[:, 0], cluster_centers_pca[:, 1], \n",
    "                c='red', marker='x', s=200, linewidths=3, label='Cluster Centers')\n",
    "    \n",
    "    plt.colorbar(scatter, label='Cluster')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/cluster_visualization_pca.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Enhanced Cluster Characteristics Heatmap\n",
    "    cluster_means = pd.DataFrame(\n",
    "        StandardScaler().fit_transform(X),\n",
    "        columns=feature_names\n",
    "    ).groupby(labels).mean()\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(cluster_means, cmap='RdYlBu', center=0, annot=True, fmt='.2f', \n",
    "                cbar_kws={'label': 'Standardized Value'})\n",
    "    plt.title('Cluster Characteristics Heatmap', pad=20)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/cluster_characteristics.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Spending Distribution by Cluster\n",
    "    if 'TotalSpend' in features_df.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(data=features_df, x='Cluster', y='TotalSpend')  # Modified this line\n",
    "        plt.title('Distribution of Total Spending by Cluster', pad=20)\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Total Spending')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/spending_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 4. Transaction Count Distribution by Cluster\n",
    "    if 'TransactionCount' in features_df.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.boxplot(data=features_df, x='Cluster', y='TransactionCount')  # Modified this line\n",
    "        plt.title('Distribution of Transaction Count by Cluster', pad=20)\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Number of Transactions')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('plots/transaction_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # 5. Add cluster size distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    cluster_sizes = features_df['Cluster'].value_counts().sort_index()\n",
    "    cluster_sizes.plot(kind='bar')\n",
    "    plt.title('Cluster Size Distribution', pad=20)\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Number of Customers')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/cluster_sizes.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return features_df  # Return the DataFrame with cluster labels\n",
    "def analyze_clusters(features_df, labels, feature_names):\n",
    "    \"\"\"\n",
    "    Perform detailed analysis of clusters\n",
    "    \"\"\"\n",
    "    # Add cluster labels to features\n",
    "    features_df['Cluster'] = labels\n",
    "    \n",
    "    # Initialize cluster analysis dictionary\n",
    "    cluster_analysis = {}\n",
    "    \n",
    "    # For each cluster\n",
    "    for cluster in range(len(set(labels))):\n",
    "        cluster_data = features_df[features_df['Cluster'] == cluster]\n",
    "        \n",
    "        # Basic statistics\n",
    "        stats = {\n",
    "            'Size': len(cluster_data),\n",
    "            'Percentage': f\"{(len(cluster_data) / len(features_df) * 100):.2f}%\"\n",
    "        }\n",
    "        \n",
    "        # Feature statistics\n",
    "        feature_stats = {}\n",
    "        for feature in feature_names:\n",
    "            if feature in cluster_data.columns:\n",
    "                feature_stats[feature] = {\n",
    "                    'Mean': cluster_data[feature].mean(),\n",
    "                    'Median': cluster_data[feature].median(),\n",
    "                    'Std': cluster_data[feature].std()\n",
    "                }\n",
    "        \n",
    "        # Combine all statistics\n",
    "        cluster_analysis[f'Cluster_{cluster}'] = {\n",
    "            'Basic_Stats': stats,\n",
    "            'Feature_Stats': feature_stats\n",
    "        }\n",
    "    \n",
    "    return cluster_analysis\n",
    "\n",
    "def generate_cluster_report(cluster_analysis, db_index, silhouette, ch_score):\n",
    "    \"\"\"\n",
    "    Generate a detailed cluster analysis report\n",
    "    \"\"\"\n",
    "    report = [\"Customer Segmentation Analysis Report\\n\", \"=\" * 40 + \"\\n\\n\"]\n",
    "    \n",
    "    # Overall metrics\n",
    "    report.append(\"Clustering Quality Metrics:\\n\")\n",
    "    report.append(f\"Davies-Bouldin Index: {db_index:.3f}\")\n",
    "    report.append(f\"Silhouette Score: {silhouette:.3f}\")\n",
    "    report.append(f\"Calinski-Harabasz Score: {ch_score:.3f}\\n\\n\")\n",
    "    \n",
    "    # Cluster-specific analysis\n",
    "    for cluster, analysis in cluster_analysis.items():\n",
    "        report.append(f\"{cluster} Analysis:\\n{'-' * 20}\\n\")\n",
    "        \n",
    "        # Basic stats\n",
    "        stats = analysis['Basic_Stats']\n",
    "        report.append(f\"Size: {stats['Size']} customers ({stats['Percentage']})\\n\")\n",
    "        \n",
    "        # Feature statistics\n",
    "        report.append(\"Key Characteristics:\\n\")\n",
    "        for feature, values in analysis['Feature_Stats'].items():\n",
    "            report.append(f\"{feature}:\")\n",
    "            report.append(f\"  Mean: {values['Mean']:.2f}\")\n",
    "            report.append(f\"  Median: {values['Median']:.2f}\")\n",
    "            report.append(f\"  Std: {values['Std']:.2f}\\n\")\n",
    "        \n",
    "        report.append(\"\\n\")\n",
    "    \n",
    "    # Write report to file\n",
    "    with open('cluster_analysis_report.txt', 'w') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    customers_df = pd.read_csv('Customers.csv')\n",
    "    transactions_df = pd.read_csv('Transactions.csv')\n",
    "    \n",
    "    # Prepare features\n",
    "    features_df, feature_names = prepare_features(customers_df, transactions_df)\n",
    "    \n",
    "    # Get feature matrix (excluding CustomerID)\n",
    "    X = features_df[feature_names].values\n",
    "    \n",
    "    # Scale features\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Find optimal number of clusters\n",
    "    k_range, db_scores, inertias, silhouette_scores, ch_scores = find_optimal_clusters(X)\n",
    "    \n",
    "    # Plot metrics\n",
    "    plot_clustering_metrics(k_range, db_scores, inertias, silhouette_scores, ch_scores)\n",
    "    \n",
    "    # Select optimal number of clusters (based on DB Index)\n",
    "    optimal_k = k_range[np.argmin(db_scores)]\n",
    "    \n",
    "    # Perform final clustering\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    \n",
    "    # Create visualizations and get updated features_df with cluster labels\n",
    "    features_df = visualize_clusters(X, labels, feature_names, features_df)\n",
    "    \n",
    "    # Perform detailed cluster analysis\n",
    "    cluster_analysis = analyze_clusters(features_df, labels, feature_names)\n",
    "    \n",
    "    # Generate detailed report\n",
    "    generate_cluster_report(\n",
    "        cluster_analysis,\n",
    "        min(db_scores),\n",
    "        silhouette_scores[optimal_k-2],\n",
    "        ch_scores[optimal_k-2]\n",
    "    )\n",
    "    \n",
    "    # Save cluster assignments\n",
    "    features_df[['CustomerID', 'Cluster']].to_csv('customer_segments.csv', index=False)\n",
    "    \n",
    "    print(\"\\nAnalysis complete! Check the following files:\")\n",
    "    print(\"1. plots/ directory for visualizations\")\n",
    "    print(\"2. cluster_analysis_report.txt for detailed analysis\")\n",
    "    print(\"3. customer_segments.csv for cluster assignments\")\n",
    "    \n",
    "    return features_df, cluster_analysis\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    features_df, cluster_analysis = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
